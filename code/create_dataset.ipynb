{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to create datasets as described in Section 5.1:\n",
    "\n",
    "Run all cells to generate the dataset file for a single climate event from CrisisMMD dataset [1] and save to disk as a csv file.\n",
    "\n",
    "##### Note: This step requires the pre-requisites specified in the Readme file of the repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing python libraries\n",
    "import tweepy # Python library for accessing the Twitter API, requires pip install tweepy\n",
    "import pandas as pd\n",
    "import requests\n",
    "import math\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting paths to required directories on disk\n",
    "\n",
    "# Set following path to file containing Twitter API keys saved in dictionary format.\n",
    "# API keys file content format: {\"api_key1\": \"xxxxxxxxx\", \"api_key2\": \"xxxxxxxx\"}\n",
    "api_keys_path = '../../Code/KEYS/api_keys_academic_access.txt' \n",
    "\n",
    "# Set following path to annotated tweets present in dataset downloaded from [1]\n",
    "labelled_data_path = '../../Data/CrisisMMD/CrisisMMD_v2.0/annotations' \n",
    "\n",
    "# Set following path to directory to store datasets created by this script\n",
    "dataset_store_path = '../../Data/TweetCredibilityDatasets' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the list of dataset file names as per the files stored in annotations folder of CrisisMMD dataset. Set the event_name and event_file_name in next cell for the dataset to be created.\n",
    "\n",
    "\n",
    "1. 'california_wildfires_final_data.tsv'\n",
    "2. 'hurricane_harvey_final_data.tsv'\n",
    "3. 'hurricane_irma_final_data.tsv'\n",
    "4. 'hurricane_maria_final_data.tsv'\n",
    "5. 'iraq_iran_earthquake_final_data.tsv'\n",
    "6. 'mexico_earthquake_final_data.tsv'\n",
    "7. 'srilanka_floods_final_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the filename for climate event for which the dataset is to be created\n",
    "event_file_name = 'california_wildfires_final_data.tsv'\n",
    "event_name = 'california_wildfires'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading api keys from text file\n",
    "keys = eval(open(api_keys_path).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details on initialising a tweepy client to make API requests can be found at [2]\n",
    "\n",
    "# Setting the bearer token for Twitter API access\n",
    "bearer_token = keys['BEARER_TOKEN']\n",
    "\n",
    "# Initialising Tweepy client for API requests\n",
    "client = tweepy.Client(bearer_token=bearer_token,\n",
    "                       return_type = requests.Response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for reading tweet ids from CrisisMMD dataset, making API requests, and saving dataset file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading tweet ids from CrisisMMD Datasets\n",
    "def read_tweet_ids(file_name):\n",
    "    return pd.read_csv(f'{labelled_data_path}/{file_name}', sep='\\t', usecols=['tweet_id'], squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to make Tweet Lookup API request\n",
    "# Following code is based on Tweepy get_tweets example [3] and [4]\n",
    "def get_tweets(dataset):\n",
    "    \n",
    "    # Reading tweet ids from dataset file\n",
    "    print(f'Reading tweet ids from dataset file for {dataset}...')\n",
    "    tweet_ids = read_tweet_ids(dataset)\n",
    "    \n",
    "    # Splitting tweet ids into multiple parts to limit each list to 100 ids or less\n",
    "    # The tweet lookup api supports request for only 100 ids in one call\n",
    "    parts = math.ceil(len(tweet_ids)/100)\n",
    "    tweet_id_parts = np.array_split(tweet_ids, parts)\n",
    "    \n",
    "    # Using get_tweets method of Tweepy to request Tweet Lookup API,\n",
    "    # which returns a list of tweets using tweet ids specified\n",
    "    print(f'Requesting tweets...')\n",
    "    responses = []\n",
    "    tweet_fields = ['author_id', 'entities', 'public_metrics', 'context_annotations']\n",
    "    for ids in tweet_id_parts:\n",
    "        response = client.get_tweets(','.join([str(id) for id in list(ids)]), tweet_fields=tweet_fields)\n",
    "        # Save data as dictionary, Extract \"data\" value from dictionary and save in list of responses        \n",
    "        responses.append(response.json()['data'])\n",
    "        \n",
    "    # Flattening list of lists to a single list of all tweet responses\n",
    "    all_tweets = [res for response in responses for res in response]\n",
    "    \n",
    "    # Transform to pandas Dataframe\n",
    "    df = pd.json_normalize(all_tweets)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to make User Lookup API request\n",
    "# Following code is based on Tweepy get_users example [5] and [6].\n",
    "def get_author_metrics(tweets_data):\n",
    "    # Reading author ids from dataset file    \n",
    "    print(f'Reading author ids...')\n",
    "    author_ids = tweets_data['author_id'].values.tolist()\n",
    "    \n",
    "    # Splitting author ids into multiple parts to limit each list to 100 ids or less\n",
    "    # The User lookup api supports request for only 100 ids in one call    \n",
    "    parts = math.ceil(len(author_ids)/100)    \n",
    "    author_id_parts = np.array_split(author_ids, parts)\n",
    "    \n",
    "    # Using get_users method of Tweepy to use User Lookup API which returns a list of users\n",
    "    print(f'Requesting user metrics...')\n",
    "    responses = []\n",
    "    user_fields = ['public_metrics']\n",
    "    for ids in author_id_parts:        \n",
    "        user_response = client.get_users(ids=','.join([str(id) for id in list(ids)]), user_fields=user_fields)\n",
    "        # Save data as dictionary, Extract \"data\" value from dictionary and save in list of responses\n",
    "        responses.append(user_response.json()['data'])\n",
    "        \n",
    "    # Flattening list of lists to a single list of all user responses\n",
    "    all_tweets = [res for response in responses for res in response]\n",
    "    \n",
    "    # Transform to pandas Dataframe\n",
    "    df = pd.json_normalize(all_tweets)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to save generated dataset files\n",
    "def save_dataset_files(df_complete, dataset_name):\n",
    "    print(f'Saving fetched tweets into dataset file for {dataset_name}...')\n",
    "    # Saving complete data to csv file in disk\n",
    "    # Note: Filename can be changed. The student id has been added here to file name,\n",
    "    # to differentiate CrisisMMD files from custom created datasets.  \n",
    "    df_complete.to_csv(f'{dataset_store_path}/21237189_{dataset_name}.csv', index=False)\n",
    "    print(f'Save complete for {dataset_name}.\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling functions to create dataset for specified event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "california_wildfires: Starting process...\n",
      "Reading tweet ids from dataset file for california_wildfires_final_data.tsv...\n",
      "Requesting tweets...\n",
      "Reading author ids...\n",
      "Requesting user metrics...\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n{event_name}: Starting process...')\n",
    "# Making API request to get tweets as per the annotated files\n",
    "tweets_data = get_tweets(event_file_name)\n",
    "# Making API request to fetch author information for each tweet\n",
    "author_data = get_author_metrics(tweets_data)\n",
    "# Combining the dataframes to create final dataset\n",
    "df_complete = pd.concat([tweets_data, author_data['public_metrics.followers_count']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fetched tweets into dataset file for california_wildfires_final_data...\n",
      "Save complete for california_wildfires_final_data.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Saving the dataset file to disk\n",
    "save_dataset_files(df_complete, event_file_name[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1] \"Crisismmd: Multimodal crisis dataset,\" [Online]. Available: https://crisisnlp.qcri.org/crisismmd\n",
    "\n",
    "[2] Tweepy. \"Examples: API v2: Authentication\". Available: https://docs.tweepy.org/en/stable/examples.html\n",
    "\n",
    "[3] Tweepy. \"Examples: API v2: Get Tweets\". Available: https://docs.tweepy.org/en/stable/examples.html\n",
    "\n",
    "[4] Tweepy. \"Tweet lookup: get_tweets\" Available: https://docs.tweepy.org/en/stable/client.html#tweepy.Client.get_tweets\n",
    "\n",
    "[5] Tweepy. \"Examples: API v2: Get Users\". Available: https://docs.tweepy.org/en/stable/examples.html\n",
    "\n",
    "[6] Tweepy. \"User lookup: get_users\" Available: https://docs.tweepy.org/en/stable/client.html#tweepy.Client.get_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
