{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to create tweet networks for different relationships and save graphs as described in Section 5.2:\n",
    "\n",
    "Run all cells to generate the four network files for a single climate event from CrisisMMD dataset [1] and save to disk as gml files. \n",
    "\n",
    "##### Note: This step requires to have dataset created using create_dataset.ipynb script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python libraries\n",
    "import networkx as nx # Python library for creating networks, requires pip install networkx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting paths to required directories on disk\n",
    "\n",
    "# Set following path to directory that stores datasets created using create_dataset.ipynb\n",
    "dataset_store_path = '../../Data/TweetCredibilityDatasets'\n",
    "\n",
    "# Set following path to directory to store the networks created for a climate event specified\n",
    "network_store_path = '../../Data/Networks' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the list of climate event names as per the files stored in annotations folder of CrisisMMD dataset. Set the event_name in next cell for specifying the dataset for which the networks are to be created.\n",
    "\n",
    "1. 'california_wildfires'\n",
    "2. 'hurricane_harvey'\n",
    "3. 'hurricane_irma'\n",
    "4. 'hurricane_maria'\n",
    "5. 'iraq_iran_earthquake'\n",
    "6. 'mexico_earthquake'\n",
    "7. 'srilanka_floods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the event name of climate event for which the networks are to be generated\n",
    "event_name = 'california_wildfires'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions to get list of nodes, list of edges for different relationships, creating and storing all four networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that reads the tweet ids from dataframe of the specified climate event dataset,\n",
    "# and returns a list of tweet ids stored as strings to be used as network nodes\n",
    "def get_network_nodes(tweets_df):\n",
    "    return [str(tweet_id) for tweet_id in tweets_df['id'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Edges: Relationship - Same Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to generate list of edges for the same author relationship\n",
    "def get_author_edges(tweets_df):\n",
    "    # Grouping tweet ids by same author\n",
    "    same_author_tweets = tweets_df[tweets_df.duplicated('author_id', keep=False\n",
    "                                         )].groupby('author_id')['id'].apply(list).reset_index()\n",
    "    \n",
    "    # Generating edges between each node that has same author\n",
    "    edge_list = []\n",
    "    for tweets in same_author_tweets['id']:\n",
    "        edge_combinations = list(itertools.combinations(tweets, 2))\n",
    "        edge_combinations = [(str(x), str(y)) for x, y in edge_combinations]\n",
    "        edge_list.append(edge_combinations)\n",
    "    \n",
    "    # Returning list of edges without duplicates stored as list of tuples,\n",
    "    # each tuple contains two tweet nodes to be connected\n",
    "    return list(set(list(itertools.chain(*edge_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Edges: Relationship - Same URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to generate list of edges for the same URL relationship\n",
    "def get_url_edges(tweets_data):    \n",
    "    # Fetching all urls contained in the tweet stored in entities.urls,\n",
    "    # and saving them as list of urls for a given tweet\n",
    "    expanded_urls = []\n",
    "    for index, row in tweets_data.iterrows():\n",
    "        if type(row['entities.urls']) == str:\n",
    "            urls_col = eval(row['entities.urls'])\n",
    "            urls = []\n",
    "            if type(urls_col) == list:\n",
    "                for item in urls_col:\n",
    "                    urls.append(item['expanded_url'])\n",
    "            expanded_urls.append(urls)\n",
    "        else:\n",
    "            expanded_urls.append([])\n",
    "    tweets_data['expanded_urls'] = expanded_urls    \n",
    "    \n",
    "    # Comparing each tweet's urls list to generate \n",
    "    # edge between all tweets that share an url\n",
    "    edge_list = []\n",
    "    for index, row in tweets_data.iterrows():\n",
    "        for url in row['expanded_urls']:\n",
    "            for idx, _row in tweets_data.iterrows():\n",
    "                if index != idx and url in _row['expanded_urls']:\n",
    "                    edge_list.append((str(row['id']), str(_row['id'])))\n",
    "    \n",
    "    # Returning list of edges without duplicates stored as list of tuples,\n",
    "    # each tuple contains two tweet nodes to be connected\n",
    "    return list(set(edge_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Edges: Relationship - Similar Retweet Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to generate list of edges for the Similar Retweet Count relationship\n",
    "def get_retweet_edges(tweets_df):\n",
    "    # Creating frequency table from retweet column\n",
    "    count_dict = dict(tweets_df['public_metrics.retweet_count'].value_counts())\n",
    "    \n",
    "    # Creating groups of similar retweet counts based on frequency table\n",
    "    splits = np.array_split(sorted(count_dict), len(set(count_dict.values())))\n",
    "    \n",
    "    # Grouping tweet ids with similar retweet counts\n",
    "    similar_retweets = []\n",
    "    for split in splits: \n",
    "        similar_counts = []\n",
    "        for index, row in tweets_df.iterrows():\n",
    "            if row['public_metrics.retweet_count'] in split:\n",
    "                similar_counts.append(str(row['id']))\n",
    "        similar_retweets.append(similar_counts)\n",
    "        \n",
    "    # Creating edge list based on similar retweet counts\n",
    "    edge_list = []\n",
    "    for tweets in similar_retweets:\n",
    "        edge_list.append(list(itertools.combinations(tweets, 2)))\n",
    "    \n",
    "    # Returning list of edges without duplicates stored as list of tuples,\n",
    "    # each tuple contains two tweet nodes to be connected\n",
    "    return list(set(list(itertools.chain(*edge_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Edges: Relationship -  Author Followers Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to generate list of edges for the Similar Tweet Author Followers Count relationship\n",
    "def get_followers_edges(tweets_df):    \n",
    "    # Creating frequency table from followers count column\n",
    "    count_dict = dict(tweets_df['public_metrics.followers_count'].value_counts())\n",
    "    \n",
    "    # Creating groups of similar followers count based on frequency table\n",
    "    splits = np.array_split(sorted(count_dict), len(set(count_dict.values())))\n",
    "    \n",
    "    # Grouping tweet ids with similar author follower counts\n",
    "    similar_retweets = []\n",
    "    for split in splits: \n",
    "        similar_counts = []\n",
    "        for index, row in tweets_df.iterrows():\n",
    "            if row['public_metrics.followers_count'] in split:\n",
    "                similar_counts.append(str(row['id']))\n",
    "        similar_retweets.append(similar_counts)\n",
    "        \n",
    "    # Creating edge list based on similar author follower counts\n",
    "    edge_list = []\n",
    "    for tweets in similar_retweets:\n",
    "        edge_list.append(list(itertools.combinations(tweets, 2)))\n",
    "        \n",
    "    # Returning list of edges without duplicates stored as list of tuples,\n",
    "    # each tuple contains two tweet nodes to be connected\n",
    "    return list(set(list(itertools.chain(*edge_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create specified network and save as a gml file\n",
    "def create_network(relationship, climate_event, event_data):    \n",
    "    # Initialising empty networkx graph\n",
    "    Tweets_G = nx.Graph()\n",
    "    \n",
    "    # Adding tweet nodes to the graph\n",
    "    print(f'\\nGetting and adding nodes to {relationship} network for {climate_event}..')\n",
    "    Tweets_G.add_nodes_from(get_network_nodes(event_data))  \n",
    "    \n",
    "    # Adding edges based on selected relationship to the graph\n",
    "    print(f'Getting and adding edges to {relationship} network for {climate_event}..')\n",
    "    if relationship == 'author':\n",
    "        Tweets_G.add_edges_from(get_author_edges(event_data))\n",
    "    elif relationship == 'url':\n",
    "        Tweets_G.add_edges_from(get_url_edges(event_data))    \n",
    "    elif relationship == 'retweet_count':\n",
    "        Tweets_G.add_edges_from(get_retweet_edges(event_data))\n",
    "    elif relationship == 'followers':\n",
    "        Tweets_G.add_edges_from(get_followers_edges(event_data))\n",
    "    else:\n",
    "        print(\"Invalid Relationship. Accepted relationships are: 'author'/'url'/'retweet_count'\")\n",
    "        return\n",
    "    \n",
    "    # Saving the graph in gml format on disk    \n",
    "    nx.write_gml(Tweets_G, f\"{network_store_path}/{climate_event}_{relationship}.gml\")\n",
    "    print(\"Graph saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset for climate event specified and generating the four networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading tweets data from csv files created using create_dataset.ipynb\n",
    "tweets_data = pd.read_csv(f'{dataset_store_path}/21237189_{event_name}_final_data.csv')\n",
    "\n",
    "# Removing duplicate tweet id rows\n",
    "tweets_data = tweets_data.copy().drop_duplicates(subset=['id']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering tweets to remove tweets with zero retweet counts as described in Section 5.2.3\n",
    "tweets_data_retweets = tweets_data.copy()[tweets_data['public_metrics.retweet_count'].values != 0]\n",
    "\n",
    "# Filtering tweets to remove tweets with zero retweet counts as described in Section 5.2.1 and 5.2.4\n",
    "tweets_data_followers = tweets_data.copy()[tweets_data['public_metrics.followers_count'].values != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting and adding nodes to author network for california_wildfires..\n",
      "Getting and adding edges to author network for california_wildfires..\n",
      "\n",
      "Getting and adding nodes to url network for california_wildfires..\n",
      "Getting and adding edges to url network for california_wildfires..\n",
      "\n",
      "Getting and adding nodes to retweet_count network for california_wildfires..\n",
      "Getting and adding edges to retweet_count network for california_wildfires..\n",
      "\n",
      "Getting and adding nodes to followers network for california_wildfires..\n",
      "Getting and adding edges to followers network for california_wildfires..\n"
     ]
    }
   ],
   "source": [
    "# Creating tweet network with same author relationship\n",
    "create_network('author', event_name, tweets_data_followers)\n",
    "\n",
    "# Creating tweet network with same url relationship\n",
    "create_network('url', event_name, tweets_data)\n",
    "\n",
    "# Creating tweet network with similar retweet count relationship\n",
    "create_network('retweet_count', event_name, tweets_data_retweets)\n",
    "\n",
    "# Creating tweet network with author followers relationship\n",
    "create_network('followers', event_name, tweets_data_followers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1] \"Crisismmd: Multimodal crisis dataset,\" [Online]. Available: https://crisisnlp.qcri.org/crisismmd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
