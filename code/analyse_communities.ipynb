{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to create network communities overlap for different relationships and save core tweets as described in Section 5.3.1.2:\n",
    "\n",
    "Run all cells to generate the four core tweet files for four overlap types for a single climate event from CrisisMMD dataset and save to disk as csv files. \n",
    "\n",
    "##### Note: This step requires to have network communties created using network_community_detection.R script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting paths to required directories on disk\n",
    "\n",
    "# Set following path to directory that stores the networks created for a climate event specified\n",
    "network_communities_path = '../../Data/Communities'\n",
    "\n",
    "# Set following path to directory that stores datasets created using create_dataset.ipynb\n",
    "dataset_store_path = '../../Data/TweetCredibilityDatasets'\n",
    "\n",
    "# Set following path to directory to store tweet overlaps\n",
    "overlap_analysis_path = '../../Data/Overlaps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the list of climate event names as per the files stored in annotations folder of CrisisMMD dataset. Set the event_name in next cell for specifying the dataset for which the networks are to be created.\n",
    "\n",
    "1. 'california_wildfires'\n",
    "2. 'hurricane_harvey'\n",
    "3. 'hurricane_irma'\n",
    "4. 'hurricane_maria'\n",
    "5. 'iraq_iran_earthquake'\n",
    "6. 'mexico_earthquake'\n",
    "7. 'srilanka_floods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the event name of climate event for which the networks are to be generated\n",
    "event_name = 'california_wildfires'\n",
    "\n",
    "# Setting community detection algorithm names\n",
    "community_algorithms = ['louvain', 'infomap', 'walktrap']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the communities generated by the three community detection algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to load communities file for the four networks from disk.\n",
    "def read_communities(algorithm, relationship):\n",
    "    return pd.read_csv(f'{network_communities_path}/{event_name}_{algorithm}_isolates_{relationship}.csv', names=\n",
    "                       ['cluster'], header=0, index_col=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading communities generated by louvain algorithm\n",
    "\n",
    "louvain_author = read_communities('louvain', 'author')\n",
    "# val[0] because louvain_author read from file has the communities stored as lists of single comma separated string \n",
    "# converting val[0] to str for cases with single tweet id in a cluster, in which case the id is saved as int64\n",
    "louvain_author = [str(val[0]).split(',') for val in louvain_author.tolist()]\n",
    "\n",
    "louvain_url = read_communities('louvain', 'urls')\n",
    "louvain_url = [str(val[0]).split(',') for val in louvain_url.tolist()]\n",
    "\n",
    "louvain_retweet_count = read_communities('louvain', 'retweet_count')\n",
    "louvain_retweet_count = [str(val[0]).split(',') for val in louvain_retweet_count.tolist()]\n",
    "\n",
    "louvain_followers = read_communities('louvain', 'followers')\n",
    "louvain_followers = [str(val[0]).split(',') for val in louvain_followers.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: 94\n",
      "URL: 11\n",
      "Retweet Counts: 13\n",
      "Follower Counts: 9\n"
     ]
    }
   ],
   "source": [
    "print(f'Author: {len(louvain_author)}')\n",
    "print(f'URL: {len(louvain_url)}')\n",
    "print(f'Retweet Counts: {len(louvain_retweet_count)}')\n",
    "print(f'Follower Counts: {len(louvain_followers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading communities generated by infomap algorithm\n",
    "\n",
    "infomap_author = read_communities('infomap', 'author')\n",
    "infomap_author = [str(val[0]).split(',') for val in infomap_author.tolist()]\n",
    "\n",
    "infomap_url = read_communities('infomap', 'urls')\n",
    "infomap_url = [str(val[0]).split(',') for val in infomap_url.tolist()]\n",
    "\n",
    "infomap_retweet_count = read_communities('infomap', 'retweet_count')\n",
    "infomap_retweet_count = [str(val[0]).split(',') for val in infomap_retweet_count.tolist()]\n",
    "\n",
    "infomap_followers = read_communities('infomap', 'followers')\n",
    "infomap_followers = [str(val[0]).split(',') for val in infomap_followers.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: 94\n",
      "URL: 11\n",
      "Retweet Counts: 13\n",
      "Follower Counts: 9\n"
     ]
    }
   ],
   "source": [
    "print(f'Author: {len(infomap_author)}')\n",
    "print(f'URL: {len(infomap_url)}')\n",
    "print(f'Retweet Counts: {len(infomap_retweet_count)}')\n",
    "print(f'Follower Counts: {len(infomap_followers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading communities generated by walktrap algorithm\n",
    "\n",
    "walktrap_author = read_communities('walktrap', 'author')\n",
    "walktrap_author = [str(val[0]).split(',') for val in walktrap_author.tolist()]\n",
    "\n",
    "walktrap_url = read_communities('walktrap', 'urls')\n",
    "walktrap_url = [str(val[0]).split(',') for val in walktrap_url.tolist()]\n",
    "\n",
    "walktrap_retweet_count = read_communities('walktrap', 'retweet_count')\n",
    "walktrap_retweet_count = [str(val[0]).split(',') for val in walktrap_retweet_count.tolist()]\n",
    "\n",
    "walktrap_followers = read_communities('walktrap', 'followers')\n",
    "walktrap_followers = [val[0].split(',') for val in walktrap_followers.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: 94\n",
      "URL: 11\n",
      "Retweet Counts: 13\n",
      "Follower Counts: 9\n"
     ]
    }
   ],
   "source": [
    "print(f'Author: {len(walktrap_author)}')\n",
    "print(f'URL: {len(walktrap_url)}')\n",
    "print(f'Retweet Counts: {len(walktrap_retweet_count)}')\n",
    "print(f'Follower Counts: {len(walktrap_followers)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A closer look at communities generated by all three algorithms shows that all algorithms are forming the same communities, hence moving forward only louvain communtities are used as discussed in Section 5.3.1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the tweet ids to integers for all communities of all four networks.\n",
    "louvain_author = [[int(val) for val in community] for community in louvain_author]\n",
    "louvain_url = [[int(val) for val in community] for community in louvain_url]\n",
    "louvain_retweet_count = [[int(val) for val in community] for community in louvain_retweet_count]\n",
    "louvain_followers = [[int(val) for val in community] for community in louvain_followers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to Get Overlapping Communities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to extract common tweets from pairwise comparison \n",
    "# of all communities in two networks at a time\n",
    "def get_overlap(communities_1, communities_2):\n",
    "    all_matches = []\n",
    "    for community_1 in communities_1:\n",
    "        for community_2 in communities_2:\n",
    "            matches = list(set(community_1) & set(community_2))\n",
    "            if len(matches) > 0:\n",
    "                all_matches.append(matches)\n",
    "    return all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the names of overlaps required as discussed in Section 5.3.1.2\n",
    "overlap_names = ['author_url_retweets', 'followers_url_retweets', 'author_followers_retweets', 'author_url_followers']\n",
    "\n",
    "# Reading tweets data from excel files created using create_dataset.ipynb \n",
    "# to be used to get tweets data for tweets extracted in overlaps and save \n",
    "# the core tweets with their complete data in overlap tweet files\n",
    "tweets_data = pd.read_csv(f'{dataset_store_path}/21237189_{event_name}_final_data.csv')    \n",
    "# Removing duplicate rows\n",
    "tweets_data = tweets_data.copy().drop_duplicates(subset=['id']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to generate and save all four overlaps\n",
    "def generate_overlaps(overlap_name):\n",
    "    \n",
    "    print(f'\\nGetting {overlap_name} overlapping community tweets')\n",
    "    \n",
    "    if overlap_name == 'author_url_retweets':\n",
    "        author_url_overlap = get_overlap(louvain_author, louvain_url)\n",
    "        overlap = get_overlap(author_url_overlap, louvain_retweet_count)\n",
    "    elif overlap_name == 'followers_url_retweets':\n",
    "        followers_url_overlap = get_overlap(louvain_followers, louvain_url)\n",
    "        overlap = get_overlap(followers_url_overlap, louvain_retweet_count)\n",
    "    elif overlap_name == 'author_followers_retweets':\n",
    "        author_follower_overlap = get_overlap(louvain_followers, louvain_author)\n",
    "        overlap = get_overlap(author_follower_overlap, louvain_retweet_count)\n",
    "    elif overlap_name == 'author_url_followers': \n",
    "        author_url_overlap = get_overlap(louvain_author, louvain_url)\n",
    "        overlap = get_overlap(author_url_overlap, louvain_followers)\n",
    "    \n",
    "    # Flattening the lists of lists obtained into a single list of core tweets\n",
    "    overlap_tweet_ids = list(itertools.chain(*overlap))\n",
    "    \n",
    "    # Saving the core tweets for overlap to disk\n",
    "    tweets_filtered = tweets_data[tweets_data['id'].isin(list(overlap_tweet_ids))].copy()\n",
    "    tweets_filtered.to_csv(f'{overlap_analysis_path}/{event_name}_{overlap_name}_tweets.csv', index=0)\n",
    "    print(f'Core tweets saved for {overlap_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting author_url_retweets overlapping community tweets\n",
      "Core tweets saved for author_url_retweets.\n",
      "\n",
      "Getting followers_url_retweets overlapping community tweets\n",
      "Core tweets saved for followers_url_retweets.\n",
      "\n",
      "Getting author_followers_retweets overlapping community tweets\n",
      "Core tweets saved for author_followers_retweets.\n",
      "\n",
      "Getting author_url_followers overlapping community tweets\n",
      "Core tweets saved for author_url_followers.\n"
     ]
    }
   ],
   "source": [
    "# Extracting community overlaps for all combinations of network relationships\n",
    "for overlap_name in overlap_names:\n",
    "    generate_overlaps(overlap_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
