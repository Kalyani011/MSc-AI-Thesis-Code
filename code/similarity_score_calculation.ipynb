{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for calculating similarity between the core tweets and other tweet texts as described in Section 5.3.2:\n",
    "\n",
    "Run all cells to generate the similarity scores and evaluation scores for a given network community overlap for a single climate event from CrisisMMD dataset [1] and save to disk.\n",
    "\n",
    "##### Note: This step requires to have the four community overlap files created using analyse_communities.ipynb script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising directory paths\n",
    "\n",
    "# Set following path to annotated tweets present in dataset downloaded from [1]\n",
    "labelled_data_path = '../../Data/CrisisMMD/CrisisMMD_v2.0/annotations' \n",
    "\n",
    "# Path to directory containg the core tweets for each overlap type created in current time\n",
    "overlap_analysis_path = '../../Data/Overlaps'\n",
    "\n",
    "# Path to dataset created in current time using create_dataset.ipynb\n",
    "dataset_store_path = '../../Data/TweetCredibilityDatasets' \n",
    "\n",
    "# Set following path to directory to store evaluation results\n",
    "evaluation_path = '../evaluation/thesis_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the list of dataset file names as per the files stored in annotations folder of CrisisMMD dataset. Set the event_name and event_file_name in next cell for running similarity score generation and evaluation for a climate event.\n",
    "\n",
    "1. 'california_wildfires_final_data.tsv'\n",
    "2. 'hurricane_harvey_final_data.tsv'\n",
    "3. 'hurricane_irma_final_data.tsv'\n",
    "4. 'hurricane_maria_final_data.tsv'\n",
    "5. 'iraq_iran_earthquake_final_data.tsv'\n",
    "6. 'mexico_earthquake_final_data.tsv'\n",
    "7. 'srilanka_floods_final_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the event name and file name of climate event for which the similarity scores are to be calculated\n",
    "\n",
    "event_name = 'california_wildfires'\n",
    "event_file_name = 'california_wildfires_final_data.tsv'\n",
    "\n",
    "# event_name = 'hurricane_harvey'\n",
    "# event_file_name = 'hurricane_harvey_final_data.tsv'\n",
    "\n",
    "# event_name = 'hurricane_irma'\n",
    "# event_file_name = 'hurricane_irma_final_data.tsv'\n",
    "\n",
    "# event_name = 'hurricane_maria'\n",
    "# event_file_name = 'hurricane_maria_final_data.tsv'\n",
    "\n",
    "# event_name = 'iraq_iran_earthquake'\n",
    "# event_file_name = 'iraq_iran_earthquake_final_data.tsv'\n",
    "\n",
    "# event_name = 'mexico_earthquake'\n",
    "# event_file_name = 'mexico_earthquake_final_data.tsv'\n",
    "\n",
    "# event_name = 'srilanka_floods'\n",
    "# event_file_name = 'srilanka_floods_final_data.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code calculates the credibility scores and evaluation scores for one tweets overlap for the event set in previous cell at a time. Enter the overlap name for which the core tweets are being loaded as the gold standard collection, against which all other tweets are scored:\n",
    "\n",
    "**Possible overlap file names:**\n",
    "1. author_url_retweets_tweets\n",
    "2. followers_url_retweets_tweets\n",
    "3. author_followers_retweets_tweets\n",
    "4. author_url_followers_tweets\n",
    "\n",
    "**Note:** For the Iraq Iran Earthquake dataset as only author_followers_retweets overlap contains core tweets and others are empty as explained in Section 7.1, the only overlap that provides results is the author_followers_retweets_tweets, other overlaps would throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter required overlap name:author_url_retweets_tweets\n"
     ]
    }
   ],
   "source": [
    "overlap_file_name = input(\"Enter required overlap name:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Score Calculation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read overlap tweets for given overlap file name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to read different set of overlapping tweets obtained from experimenting\n",
    "# with different relationship networks in analyse_communities.ipynb.\n",
    "def read_overlap_tweets(filename):\n",
    "    return pd.read_csv(f'{overlap_analysis_path}/{event_name}_{filename}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_tweets = read_overlap_tweets(overlap_file_name).copy()\n",
    "# Setting the core tweets corpus to tweet texts\n",
    "corpus = overlap_tweets['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating tf-idf scores for tweet terms of tweet texts in the set of core tweets given by the community overlap file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following use of tf-idf vectorizer is based on [2] and [3]\n",
    "# The TfidfVectorizer has parameters that provide text pre-processing such as stop word removal, \n",
    "# use of different tokenizers, and using either word or character n-grams. \n",
    "# The following setting was found to be working the best for all the given datasets, \n",
    "# after experimenting with different parameters\n",
    "\n",
    "# Initialising the vectorizer for generating character n-gram features\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "n_grams = '2_4'\n",
    "# Generating character n-gram features as character bigram, trigram and four-gram tf-idf values\n",
    "# using the core tweets obtained in the overlap as the vocabulary\n",
    "corpus_tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Similarity of all Tweets with respect to core tweets using COSINE Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to read labelled data from original CrisisMMD files for tweets in the dataset generated in current time\n",
    "def read_labelled_tweets(file_name):\n",
    "    # Reading tweets data from csv file created using create_dataset.ipynb\n",
    "    tweets_data = pd.read_csv(f'{dataset_store_path}/21237189_{event_name}_final_data.csv')\n",
    "    # Removing duplicate tweet id rows\n",
    "    tweets_data = tweets_data.drop_duplicates(subset=['id']).reset_index()\n",
    "    # Reading annotated tweets from CrisisMMD annotations file for given climate event\n",
    "    # text_info contains the informative/not informative labels\n",
    "    annotated_tweets = pd.read_csv(f'{labelled_data_path}/{file_name}', sep='\\t', usecols=\n",
    "                       ['tweet_id', 'text_info', 'tweet_text'], squeeze=True)\n",
    "    return annotated_tweets[annotated_tweets['tweet_id'].isin(tweets_data['id'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all tweets in dataset along with their informative/not informative labels \n",
    "# for calculating credibility scores for all tweets in the dataset\n",
    "test_tweets = read_labelled_tweets(event_file_name).copy()\n",
    "# Removing Duplicates\n",
    "test_tweets = test_tweets.drop_duplicates(subset=['tweet_id'])\n",
    "test_corpus = test_tweets['tweet_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Tf-Idf representation of all tweets based on overlapping tweet corpus\n",
    "test_tfidf = vectorizer.transform(test_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting cosine similarity scores of all tweets in dataset with all tweets in the core tweet set\n",
    "similarities = cosine_similarity(test_tfidf, corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking mean similarity of each tweet with respect to its similarity to all core tweets\n",
    "mean_similarities = []\n",
    "for similarity in similarities:\n",
    "    mean_similarities.append(np.mean(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the similarities along with corresponding tweet ids\n",
    "tweet_scores = {}\n",
    "for index, similarity in enumerate(mean_similarities):\n",
    "    tweet_scores[test_tweets['tweet_id'].values[index]] = similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43730"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving similarity scores to disk\n",
    "open(f\"{evaluation_path}/{event_name}/{overlap_file_name[:-7]}_similarity_scores_{n_grams}.txt\", \"w\"\n",
    "        ).write(repr(tweet_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining predictions based on threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting threshold to mean of similarities\n",
    "threshold = np.mean(mean_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the predictions based on threshold along with the actual labels for each tweet\n",
    "predictions = {}\n",
    "for key, value in tweet_scores.items():\n",
    "    actual_label = test_tweets[test_tweets['tweet_id']==key]['text_info'].values[0]\n",
    "    if value > threshold:        \n",
    "        predictions[key] = ['informative', actual_label]\n",
    "    else:\n",
    "        predictions[key] = ['not_informative', actual_label]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the precision, recall, and f1 scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Scikit Learn Metrics functions\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Method to run evaluation metrics and save results on disk\n",
    "def evaluation_metrics(predictions):    \n",
    "    \n",
    "    # Separating actual labels from predicted labels    \n",
    "    predicted_label = np.array(list(predictions.values()))[:, 0]\n",
    "    actual_label = np.array(list(predictions.values()))[:, 1]\n",
    "    \n",
    "    # Getting the confusion matrix for the predictions\n",
    "    conf_mat = confusion_matrix(actual_label, predicted_label, labels=[\"informative\", \"not_informative\"])\n",
    "    confusion_mat = {}\n",
    "    confusion_mat['informative'] = conf_mat[0]\n",
    "    confusion_mat['not_informative'] = conf_mat[1]\n",
    "    confusion_mat['True Positives'] = conf_mat.ravel()[0]\n",
    "    confusion_mat['False Negatives'] = conf_mat.ravel()[1]\n",
    "    confusion_mat['False Positives'] = conf_mat.ravel()[2]\n",
    "    confusion_mat['True Negatives'] = conf_mat.ravel()[3]\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(pd.json_normalize(confusion_mat))\n",
    "    \n",
    "    # Saving the confusion matrix to file on disk\n",
    "    open(f\"{evaluation_path}/{event_name}/{overlap_file_name[:-7]}_confusion_matrix_{n_grams}.txt\", \"w\"\n",
    "        ).write(repr(confusion_mat))\n",
    "    \n",
    "    # Getting Classification Report   \n",
    "    classification_rep = classification_report(actual_label, predicted_label, target_names=[\"informative\", \"not_informative\"])\n",
    "    print(\"\\n Classification Report\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Getting individual precision, recall and f1 score for relevant class\n",
    "    all_metrics = {}\n",
    "    all_metrics['precision'] = round(precision_score(actual_label, predicted_label, pos_label='informative'), 4)\n",
    "    all_metrics['recall'] = round(recall_score(actual_label, predicted_label, pos_label='informative'), 4)\n",
    "    all_metrics['f1'] = round(f1_score(actual_label, predicted_label, pos_label='informative'), 4)\n",
    "    \n",
    "    print(\"\\nEvaluation Scores for Relevant Tweets Retrieved:\")\n",
    "    print(pd.json_normalize(all_metrics))\n",
    "    \n",
    "    # Saving the evaluation scores to file on disk\n",
    "    open(f\"{evaluation_path}/{event_name}/{overlap_file_name[:-7]}_evaluation_scores_{n_grams}.txt\", \"w\"\n",
    "        ).write(repr(all_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evaluation for current set of predictions obtained for a given community overlap, for a given climate event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "  informative not_informative  True Positives  False Negatives  \\\n",
      "0  [577, 268]      [106, 133]             577              268   \n",
      "\n",
      "   False Positives  True Negatives  \n",
      "0              106             133  \n",
      "\n",
      " Classification Report\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    informative       0.84      0.68      0.76       845\n",
      "not_informative       0.33      0.56      0.42       239\n",
      "\n",
      "       accuracy                           0.65      1084\n",
      "      macro avg       0.59      0.62      0.59      1084\n",
      "   weighted avg       0.73      0.65      0.68      1084\n",
      "\n",
      "\n",
      "Evaluation Scores for Relevant Tweets Retrieved:\n",
      "   precision  recall      f1\n",
      "0     0.8448  0.6828  0.7552\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1] \"Crisismmd: Multimodal crisis dataset,\" [Online]. Available: https://crisisnlp.qcri.org/crisismmd \n",
    "\n",
    "[2] U. Malik. \"Python for NLP: Sentiment Analysis with Scikit-Learn,\" 2022. Available: https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/\n",
    "\n",
    "[3] Scikit-Learn. \"sklearn.feature_extraction.text.TfidfVectorizer\", Available: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
