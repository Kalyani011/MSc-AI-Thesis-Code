{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for checking if the tweets in set of core tweets are labelled informative or not informative in CrisisMMD dataset as described in Section 5.3.1.2:\n",
    "\n",
    "Run all cells to check the ground truth labels for all network community overlaps for a single climate event from CrisisMMD dataset [1] and save to disk.\n",
    "\n",
    "##### Note: This step requires to have the four community overlap files created using analyse_communities.ipynb script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising directory paths\n",
    "\n",
    "# Set following path to annotated tweets present in dataset downloaded from [1]\n",
    "labelled_data_path = '../../Data/CrisisMMD/CrisisMMD_v2.0/annotations'\n",
    "\n",
    "# Path to directory containg the core tweets for each overlap type created in current time\n",
    "overlap_analysis_path = '../../Data/Overlaps'\n",
    "\n",
    "# Path to dataset created in current time using create_dataset.ipynb\n",
    "dataset_store_path = '../../Data/TweetCredibilityDatasets'\n",
    "\n",
    "# Path to save results of the analysis\n",
    "result_path = '../evaluation/reliability_check'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the list of dataset file names as per the files stored in annotations folder of CrisisMMD dataset. Set the event_name and event_file_name in next cell for checking reliability of tweets in core tweets sets for a climate event.\n",
    "\n",
    "1. 'california_wildfires_final_data.tsv'\n",
    "2. 'hurricane_harvey_final_data.tsv'\n",
    "3. 'hurricane_irma_final_data.tsv'\n",
    "4. 'hurricane_maria_final_data.tsv'\n",
    "5. 'iraq_iran_earthquake_final_data.tsv'\n",
    "6. 'mexico_earthquake_final_data.tsv'\n",
    "7. 'srilanka_floods_final_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the event name and file name of climate event for which the similarity scores are to be calculated\n",
    "event_name = 'california_wildfires'\n",
    "event_file_name = 'california_wildfires_final_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the file names for overlaps containing core tweets for a given event\n",
    "overlap_files = ['author_url_retweets_tweets', \n",
    "                 'followers_url_retweets_tweets', \n",
    "                 'author_followers_retweets_tweets', \n",
    "                 'author_url_followers_tweets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions to read core tweets, annotated tweets and checking the labels of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to read tweets in the set of core tweets for a given overlap type\n",
    "def read_overlap_tweets(filename):\n",
    "    return pd.read_csv(f'{overlap_analysis_path}/{event_name}_{filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to read annotated files\n",
    "def read_labelled_tweets(file_name, event_name):\n",
    "    # Reading tweets data from csv files created using create_dataset.ipynb\n",
    "    tweets_data = pd.read_csv(f'{dataset_store_path}/21237189_{event_name}_final_data.csv')    \n",
    "    # Removing duplicate rows\n",
    "    tweets_data = tweets_data.drop_duplicates(subset=['id']).reset_index()\n",
    "    annotated_tweets = pd.read_csv(f'{labelled_data_path}/{file_name}', sep='\\t', usecols=\n",
    "                       ['tweet_id', 'text_info', 'tweet_text'], squeeze=True)\n",
    "    return annotated_tweets[annotated_tweets['tweet_id'].isin(tweets_data['id'].values)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to check the labels of core tweets and \n",
    "# extract the number of informative and not informative tweets captured in each core tweets set\n",
    "def check_informativeness(overlap_file_name):\n",
    "    informativeness_results = {}\n",
    "    overlap_tweets = read_overlap_tweets(overlap_file_name).copy()\n",
    "    overlap_tweet_ids = overlap_tweets['id'].values\n",
    "    info_matches = set(overlap_tweet_ids) & set(informative_clusters['tweet_id'].values[0])\n",
    "    not_info_matches = set(overlap_tweet_ids) & set(informative_clusters['tweet_id'].values[1])\n",
    "    informativeness_results['community overlap name'] = overlap_file_name    \n",
    "    informativeness_results['total tweets'] = len(info_matches) + len(not_info_matches)\n",
    "    informativeness_results['informative tweets'] = len(info_matches)\n",
    "    informativeness_results['not informative tweets'] = len(not_info_matches)\n",
    "    return informativeness_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the tweet reliability check for given climate event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all tweets in dataset along with their informative/not informative labels \n",
    "labelled_tweets = read_labelled_tweets(event_file_name, event_name)\n",
    "labelled_tweets = labelled_tweets.drop_duplicates(subset=['tweet_id'])\n",
    "\n",
    "# Grouping tweets by their informativeness label\n",
    "informative_clusters = labelled_tweets[labelled_tweets.duplicated('text_info', keep=False\n",
    "                                         )].groupby('text_info')['tweet_id'].apply(list).reset_index()\n",
    "# Running the reliability check for all core tweet sets for a given climate event\n",
    "results = []\n",
    "for overlap in overlap_files:\n",
    "    results.append(check_informativeness(overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | community overlap name           |   total tweets |   informative tweets |   not informative tweets |\n",
      "|---:|:---------------------------------|---------------:|---------------------:|-------------------------:|\n",
      "|  0 | author_url_retweets_tweets       |              3 |                    3 |                        0 |\n",
      "|  1 | followers_url_retweets_tweets    |              4 |                    4 |                        0 |\n",
      "|  2 | author_followers_retweets_tweets |             69 |                   59 |                       10 |\n",
      "|  3 | author_url_followers_tweets      |             41 |                   27 |                       14 |\n"
     ]
    }
   ],
   "source": [
    "# Saving the results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_markdown())\n",
    "results_df.to_csv(f'{result_path}/{event_name}_informativeness_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1] \"Crisismmd: Multimodal crisis dataset,\" [Online]. Available: https://crisisnlp.qcri.org/crisismmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
