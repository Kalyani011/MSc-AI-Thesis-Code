{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to apply the supervised approach as described in Section 5.4.1:\n",
    "\n",
    "All the preprocessing, use of classifiers, and hyperparameter settings are based on the research done by [1].\n",
    "\n",
    "Run all cells to apply the supervised approach to CrisisMMD dataset.\n",
    "\n",
    "##### Note: This step requires to have dataset created using create_dataset.ipynb script to filter out tweets from CrisisMMD dataset that do not exist in real-time. The approach may be applied to CrisisMMD directly as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising directory paths\n",
    "\n",
    "# Set following path to annotated tweets present in dataset downloaded from [1]\n",
    "labelled_data_path = '../../Data/CrisisMMD/CrisisMMD_v2.0/annotations'\n",
    "\n",
    "# Path to dataset created in current time by create_dataset.ipynb\n",
    "dataset_store_path = '../../Data/TweetCredibilityDatasets'\n",
    "\n",
    "# Path to directory to store evaluation results of supervised approach\n",
    "evaluation_path = '../evaluation/supervised_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the list of dataset file names as per the files stored in annotations folder of CrisisMMD dataset. Set the event_name and event_file_name in next cell for running the supervised approach and save corresponding evaluations for a climate event.\n",
    "\n",
    "1. 'california_wildfires_final_data.tsv'\n",
    "2. 'hurricane_harvey_final_data.tsv'\n",
    "3. 'hurricane_irma_final_data.tsv'\n",
    "4. 'hurricane_maria_final_data.tsv'\n",
    "5. 'iraq_iran_earthquake_final_data.tsv'\n",
    "6. 'mexico_earthquake_final_data.tsv'\n",
    "7. 'srilanka_floods_final_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the event name and file name of climate event for which the similarity scores are to be calculated\n",
    "event_name = 'california_wildfires'\n",
    "event_file_name = 'california_wildfires_final_data.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for text preprocessing and reading tweets from dataset files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following code uses [2] for removing url links as also done in [1]\n",
    "# All pre-processing performed is as per the processing steps described in [1]\n",
    "\n",
    "# Initialising text processing libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Initialising stemmer to extract word stems\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Initialising tokenizer for tweets using nltk tweet_tokenizer\n",
    "tweet_tokenizer = TweetTokenizer() \n",
    "\n",
    "def tokenize_tweets(corpus):\n",
    "    tokenized_corpus = []\n",
    "    for tweet in corpus:\n",
    "        # Converting text into tokens\n",
    "        tokens = tweet_tokenizer.tokenize(tweet)\n",
    "        # Removing stop words and stemming\n",
    "        processed_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stopwords.words('English')]\n",
    "        # Removing url tokens\n",
    "        processed_tokens = [re.sub(r\"http\\S+\", '', token) for token in processed_tokens]\n",
    "        # Removing special characters\n",
    "        processed_tokens = [re.sub('\\W+','', token) for token in processed_tokens]\n",
    "        # Recreating the text by joining tokens\n",
    "        tokenized_corpus.append(' '.join(processed_tokens).strip())\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to read labelled data from original CrisisMMD files for tweets in the dataset generated in current time\n",
    "\n",
    "def read_labelled_tweets(file_name):\n",
    "    # Reading tweets data from excel files created using create_dataset.ipynb\n",
    "    tweets_data = pd.read_csv(f'{dataset_store_path}/21237189_{event_name}_final_data.csv')    \n",
    "    # Removing duplicate rows\n",
    "    tweets_data = tweets_data.drop_duplicates(subset=['id']).reset_index()\n",
    "    annotated_tweets = pd.read_csv(f'{labelled_data_path}/{file_name}', sep='\\t', usecols=\n",
    "                       ['tweet_id', 'text_info', 'tweet_text'], squeeze=True)\n",
    "    return annotated_tweets[annotated_tweets['tweet_id'].isin(tweets_data['id'].values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and processing tweet texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all tweets in dataset along with their informative/not informative labels \n",
    "tweets = read_labelled_tweets(event_file_name).copy()\n",
    "# Removing Duplicates\n",
    "tweets = tweets.drop_duplicates(subset=['tweet_id'])\n",
    "# Setting the corpus\n",
    "corpus = tweets['tweet_text'].values\n",
    "# Preprocessing the corpus\n",
    "processed_corpus = tokenize_tweets(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating TF-IDF features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the vectorizer for generating word n-gram features\n",
    "# [1] claims that the unigrams and bigrams provide the best results,\n",
    "# thus setting the parameters accordingly\n",
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "n_grams = '1_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process applied to complete dataset as also done in [1]\n",
    "tweets_tfidf = vectorizer.fit_transform(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the output labels to binary values\n",
    "tweet_labels = [1 if tweet_label=='informative' else 0 for tweet_label in tweets['text_info'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As stated in [1] \"The implementation of the classifiers in scikitlearn python library,\n",
    "# is used setting all the parameters to the default values\", the following function is designed,\n",
    "# to run all classifiers on the dataset with their default settings using 10-fold cross validation.\n",
    "\n",
    "def run_classifiers(X, y, n_grams):\n",
    "    results = []\n",
    "    \n",
    "    lsvm_result = {}\n",
    "    lsvm_result['classifier'] = 'Linear SVM'\n",
    "    lsvm_clf = LinearSVC()\n",
    "    lsvm_result['precision'] = np.mean(cross_val_score(lsvm_clf, X, y, cv=10, scoring='precision'))\n",
    "    lsvm_result['recall'] = np.mean(cross_val_score(lsvm_clf, X, y, cv=10, scoring='recall'))\n",
    "    lsvm_result['f1'] = np.mean(cross_val_score(lsvm_clf, X, y, cv=10, scoring='f1'))\n",
    "    results.append(lsvm_result)\n",
    "    \n",
    "    lr_result = {}\n",
    "    lr_result['classifier'] = 'Logistic Regression'\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_result['precision'] = np.mean(cross_val_score(lr_clf, X, y, cv=10, scoring='precision'))\n",
    "    lr_result['recall'] = np.mean(cross_val_score(lr_clf, X, y, cv=10, scoring='recall'))\n",
    "    lr_result['f1'] = np.mean(cross_val_score(lr_clf, X, y, cv=10, scoring='f1'))\n",
    "    results.append(lr_result)\n",
    "    \n",
    "    rf_result = {}\n",
    "    rf_result['classifier'] = 'Random Forest'\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    rf_result['precision'] = np.mean(cross_val_score(rf_clf, X, y, cv=10, scoring='precision'))\n",
    "    rf_result['recall'] = np.mean(cross_val_score(rf_clf, X, y, cv=10, scoring='recall'))\n",
    "    rf_result['f1'] = np.mean(cross_val_score(rf_clf, X, y, cv=10, scoring='f1'))\n",
    "    results.append(rf_result)\n",
    "    \n",
    "    nb_result = {}\n",
    "    nb_result['classifier'] = 'Naive Bayes'\n",
    "    nb_clf = GaussianNB()\n",
    "    nb_result['precision'] = np.mean(cross_val_score(nb_clf, X, y, cv=10, scoring='precision'))\n",
    "    nb_result['recall'] = np.mean(cross_val_score(nb_clf, X, y, cv=10, scoring='recall'))\n",
    "    nb_result['f1'] = np.mean(cross_val_score(nb_clf, X, y, cv=10, scoring='f1'))\n",
    "    results.append(nb_result)\n",
    "    \n",
    "    knn_result = {}\n",
    "    knn_result['classifier'] = 'K-Nearest Neighbour'\n",
    "    knn_clf = KNeighborsClassifier()\n",
    "    knn_result['precision'] = np.mean(cross_val_score(knn_clf, X, y, cv=10, scoring='precision'))\n",
    "    knn_result['recall'] = np.mean(cross_val_score(knn_clf, X, y, cv=10, scoring='recall'))\n",
    "    knn_result['f1'] = np.mean(cross_val_score(knn_clf, X, y, cv=10, scoring='f1'))\n",
    "    results.append(knn_result)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    results_df.to_csv(f\"{evaluation_path}/{event_name}_supervised_scores_{n_grams}.csv\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | classifier          |   precision |   recall |     f1 |\n",
      "|---:|:--------------------|------------:|---------:|-------:|\n",
      "|  0 | Linear SVM          |      0.7842 |   0.9929 | 0.8763 |\n",
      "|  1 | Logistic Regression |      0.7795 |   1      | 0.8761 |\n",
      "|  2 | Random Forest       |      0.78   |   0.9988 | 0.8749 |\n",
      "|  3 | Naive Bayes         |      0.8057 |   0.8354 | 0.82   |\n",
      "|  4 | K-Nearest Neighbour |      0.7933 |   0.9539 | 0.8661 |\n"
     ]
    }
   ],
   "source": [
    "results = run_classifiers(tweets_tfidf.toarray(), tweet_labels, n_grams)\n",
    "print(results.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1] N. Hassan, W. Gomaa, G. Khoriba, and M. Haggag, “Credibility detection in twitter using word n-gram analysis and supervised machine learning techniques,” International Journal of Intelligent Engineering and Systems, vol. 13, pp. 291–300, Dec. 2020. [Online]. Available: https://doi.org/10.22266/ijies2020.0229.27\n",
    "\n",
    "[2] \"Expression to remove URL links from Twitter tweet,\" 2021. Available: https://stackoverflow.com/questions/24399820/expression-to-remove-url-links-from-twitter-tweet\n",
    "\n",
    "[3] Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
